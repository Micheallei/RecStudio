dropout: 0.5
hidden_dim: 200
latent_dim: 200
activation: relu

gamma: 0.005
beta: 0.005

alternating: True
enc_epoch: 3
dec_epoch: 1

learning_rate: 0.001
weight_decay: 1e-5
learner: adam
epochs: 200
batch_size: 256
eval_batch_size: 200
early_stop_patience: 100